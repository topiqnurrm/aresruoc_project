#  -----------------------------------------------------------------------------------------
#  (C) Copyright IBM Corp. 2024.
#  https://opensource.org/licenses/BSD-3-Clause
#  -----------------------------------------------------------------------------------------
from __future__ import annotations

__all__ = [
    "DocumentsIterableDataset",
]

import os
from copy import copy
import pandas as pd
from typing import TYPE_CHECKING, Iterator, Any

from ibm_watsonx_ai.wml_client_error import WMLClientError
from ibm_watsonx_ai.data_loaders.text_loader import (
    TextLoader,
    _asynch_download,
    _sequential_download,
)
from ibm_watsonx_ai.helpers.remote_document import RemoteDocument
from ibm_watsonx_ai.utils.autoai.enums import DocumentsSamplingTypes, SamplingTypes
from ibm_watsonx_ai.utils.autoai.errors import InvalidSizeLimit

if TYPE_CHECKING:
    from ibm_watsonx_ai.helpers.connections import DataConnection

# Note: try to import torch lib if available, this fallback is done based on
# torch dependency removal request
try:
    from torch.utils.data import IterableDataset

except ImportError:
    IterableDataset: type = object  # type: ignore[no-redef]
# --- end note

DEFAULT_SAMPLE_SIZE_LIMIT = (
    1073741824  # 1GB in Bytes is verified later by _set_sample_size_limit
)
DEFAULT_REDUCED_SAMPLE_SIZE_LIMIT = 104857600  # 100MB in bytes
DEFAULT_SAMPLING_TYPE = SamplingTypes.FIRST_VALUES
DEFAULT_DOCUMENTS_SAMPLING_TYPE = DocumentsSamplingTypes.RANDOM


class DocumentsIterableDataset(IterableDataset):
    """
    This dataset is intended to be an Iterable stream of documents using underneath Flight Service.
    It can download documents asynchronously and serve them to the user from generator.

    :param connections: list of connections to documents
    :type connections: list[DataConnection]

    :param enable_sampling: if set to `True`, will enable sampling, default: True
    :type enable_sampling: bool

    :param sample_size_limit: upper limit for documents that should be downloaded in bytes, default: 1 GB
    :type sample_size_limit: int

    :param sampling_type: a sampling strategy how to read the data,
        check `DocumentsSamplingTypes` enum class for more options
    :type sampling_type: str

    :param total_size_limit: upper limit for documents that should be downloaded in Bytes, default: 1GB,
        if more than one of: `total_size_limit`, `total_ndocs_limit` are set,
        then data are limited to the lower threshold.
    :type total_size_limit: int

    :param total_ndocs_limit: upper limit for documents that should be downloaded in number of rows,
        if more than one of: `total_size_limit`, `total_nrows_limit` are set,
        then data are limited to the lower threshold.
    :type total_ndocs_limit: int, optional

    :param benchmark_dataset: dataset of benchmarking data with ids in `document_ids` column corresponding
        to names of documents in `connections` list
    :type benchmark_dataset: pd.DataFrame, optional

    **Example: default sampling - read up to 1GB of random documents**

        .. code-block:: python

            connections = [DataConnection(data_asset_id='5d99c11a-2060-4ef6-83d5-dc593c6455e2')]

            iterable_dataset = DocumentsIterableDataset(connections=connections,
                                                        enable_sampling=True,
                                                        sampling_type='random',
                                                        sample_size_limit = 1GB)

    **Example: read all documents / no subsampling**

        .. code-block:: python

            connections = [DataConnection(data_asset_id='5d99c11a-2060-4ef6-83d5-dc593c6455e2')]

            iterable_dataset = DocumentsIterableDataset(connections=connections,
                                                        enable_sampling=False)

    **Example: context based sampling**

            .. code-block:: python

                connections = [DataConnection(data_asset_id='5d99c11a-2060-4ef6-83d5-dc593c6455e2')]

                iterable_dataset = DocumentsIterableDataset(connections=connections,
                                                            enable_sampling=True,
                                                            sampling_type='benchmark_driven',
                                                            sample_size_limit = 1GB,
                                                            benchmark_dataset=pd.DataFrame(
                                                                data={
                                                                    "question": [
                                                                        "What foundation models are available in watsonx.ai ?"
                                                                    ],
                                                                    "correct_answers": [
                                                                        [
                                                                            "The following models are available in watsonx.ai: ..."
                                                                        ]
                                                                    ],
                                                                    "correct_answer_document_ids": ["sample_pdf_file.pdf"],
                                                                }))

    """

    def __init__(
        self,
        *,
        connections: list[DataConnection],
        enable_sampling: bool = True,
        sample_size_limit: int = DEFAULT_SAMPLE_SIZE_LIMIT,
        sampling_type: str = DEFAULT_DOCUMENTS_SAMPLING_TYPE,
        total_size_limit: int = DEFAULT_SAMPLE_SIZE_LIMIT,
        total_ndocs_limit: int | None = None,
        benchmark_dataset: pd.DataFrame | None = None,
        **kwargs: Any,
    ) -> None:
        from ibm_watsonx_ai.helpers import S3Location, AssetLocation

        super().__init__()
        self.enable_sampling = enable_sampling
        self.sample_size_limit = sample_size_limit
        self.sampling_type = sampling_type
        self._set_size_limit(total_size_limit)
        self.total_ndocs_limit = total_ndocs_limit
        self.benchmark_dataset = benchmark_dataset

        self._download_strategy = kwargs.get(
            "_download_strategy", "n_parallel"
        )  # expected values: "n_parallel", "sequential_in_parallel", "sequential"

        api_client = kwargs.get("_api_client")

        if api_client is not None:
            for conn in connections:
                if conn._api_client is None:
                    conn.set_client(api_client)

        set_of_api_clients = set([conn._api_client for conn in connections])
        data_asset_id_name_mapping = {}

        if any([isinstance(conn.location, AssetLocation) for conn in connections]):

            for client in set_of_api_clients:
                for res in client.data_assets.get_details(get_all=True)["resources"]:
                    data_asset_id_name_mapping[res["metadata"]["asset_id"]] = res[
                        "metadata"
                    ]["resource_key"].split("/")[-1]

        def get_document_id(conn):
            if (
                isinstance(conn.location, AssetLocation)
                and conn.location.id in data_asset_id_name_mapping
            ):
                return data_asset_id_name_mapping.get(conn.location.id)
            elif hasattr(conn.location, "file_name"):
                return conn.location.file_name.split("/")[-1]
            elif hasattr(conn.location, "path"):
                return conn.location.path.split("/")[-1]
            else:
                raise WMLClientError(
                    "Unsupported connection type for extracting document id."
                )

        self.remote_documents = []

        for connection in connections:
            if isinstance(connection.location, S3Location):
                self.remote_documents.extend(
                    [
                        RemoteDocument(connection=c, document_id=get_document_id(c))
                        for c in connection._get_connections_from_bucket()
                    ]
                )
            else:
                self.remote_documents.append(
                    RemoteDocument(
                        connection=connection, document_id=get_document_id(connection)
                    )
                )

        if len(set([doc.document_id for doc in self.remote_documents])) < len(
            self.remote_documents
        ):
            raise WMLClientError(
                "Not unique document file names passed in connections."
            )

    def _set_size_limit(self, size_limit: int) -> None:
        """If non-default value of total_size_limit was not passed,
        set Sample Size Limit based on T-Shirt size if code is run on training pod:
        For memory < 16 (T-Shirts: XS,S) default is 10MB,
        For memory < 32 & >= 16 (T-Shirts: M) default is 100MB,
        For memory = 32 (T-Shirt L) default is 0.7GB,
        For memory > 32 (T-Shirt XL) or runs outside pod default is 1GB.
        """
        self.total_size_limit: int | None
        from ibm_watsonx_ai.utils.autoai.connection import get_max_sample_size_limit

        max_tshirt_size_limit = (
            get_max_sample_size_limit() if os.getenv("MEM", False) else None
        )  # limit manual setting of sample size limit on autoai clusters #31527

        if self.enable_sampling:
            if max_tshirt_size_limit:
                if (
                    size_limit > max_tshirt_size_limit
                    and size_limit != DEFAULT_SAMPLE_SIZE_LIMIT
                ):
                    raise InvalidSizeLimit(size_limit, max_tshirt_size_limit)
                else:
                    self.total_size_limit = min(size_limit, max_tshirt_size_limit)
            else:
                self.total_size_limit = size_limit
        else:
            if size_limit == DEFAULT_SAMPLE_SIZE_LIMIT:
                self.total_size_limit = None  # do not limit reading if sampling is disabled, we want read all data
            else:
                self.total_size_limit = size_limit

    @staticmethod
    def _docs_context_sampling(
        remote_documents: list[RemoteDocument],
        benchmark_document_ids: list[str],
    ) -> list[RemoteDocument]:
        """Randomly sample documents from benchmark set first, then randomly from the rest up to a `size_upper_bound`.

        :param remote_documents: documents to sample from
        :type remote_documents: list[RemoteDocument]

        :param benchmark_document_ids: IDs of documents from benchmark dataset
        :type benchmark_document_ids: list[str]

        :return: list of sampled documents
        :rtype: list[RemoteDocument]
        """
        sampled_documents = []
        benchmark_documents = []
        non_benchmark_documents = []

        for doc in remote_documents:
            if doc.document_id in benchmark_document_ids:
                benchmark_documents.append(doc)
            else:
                non_benchmark_documents.append(doc)

        sampled_documents.extend(
            DocumentsIterableDataset._docs_random_sampling(
                benchmark_documents,
            )
        )
        sampled_documents.extend(
            DocumentsIterableDataset._docs_random_sampling(
                non_benchmark_documents,
            )
        )

        return sampled_documents

    @staticmethod
    def _docs_random_sampling(
        remote_documents: list[RemoteDocument],
    ) -> list[RemoteDocument]:
        """Randomly sample documents from `remote_documents` up to `size_upper_bound`.

        :param remote_documents: documents to sample from
        :type remote_documents: list[RemoteDocument]

        :return: list of sampled documents
        :rtype: list[RemoteDocument]
        """
        from random import shuffle

        sampling_order = list(range(len(remote_documents)))
        shuffle(sampling_order)

        return [remote_documents[i] for i in sampling_order]

    def __iter__(self) -> Iterator:
        """Iterate over documents."""
        size_limit = (
            self.sample_size_limit
            if self.sample_size_limit is not None
            else self.total_size_limit
        )

        if self.enable_sampling:
            if self.sampling_type == DocumentsSamplingTypes.RANDOM:
                sampled_docs = self._docs_random_sampling(self.remote_documents)
            elif self.sampling_type == DocumentsSamplingTypes.BENCHMARK_DRIVEN:
                if self.benchmark_dataset is not None:
                    benchmark_documents_ids = list(
                        set(self.benchmark_dataset["correct_answer_document_ids"])
                    )
                else:
                    raise ValueError(
                        "`benchmark_dataset` is mandatory for sample_type: DocumentsSamplingTypes.BENCHMARK_DRIVEN."
                    )

                sampled_docs = self._docs_context_sampling(
                    self.remote_documents, benchmark_documents_ids
                )
            else:
                raise ValueError(
                    f"Unsupported documents sampling type: {self.sampling_type}"
                )
        else:
            sampled_docs = copy(self.remote_documents)

        if self.total_ndocs_limit is not None:
            sampled_docs = sampled_docs[: self.total_ndocs_limit]

        match self._download_strategy:
            case (
                "sequential_in_parallel"
            ):  # downloading documents sequentially but in separate thread
                import multiprocessing.dummy as mp

                q = mp.Queue()

                p = mp.Process(target=_sequential_download, args=[q, sampled_docs])
                p.start()
                res_size = 0
                for _ in range(len(sampled_docs)):
                    doc = q.get()
                    res_size += len(doc.page_content.encode("utf-8"))

                    if res_size > size_limit:
                        return

                    yield doc
                p.join()

            case "n_parallel":  # downloading documents entirely in parallel
                import multiprocessing.dummy as mp

                thread_no = min(5, len(sampled_docs))

                q_input = mp.Queue()
                qs_output = [mp.Queue() for _ in range(len(sampled_docs))]
                args = [(q_input, qs_output)] * thread_no

                for i, doc in enumerate(sampled_docs):
                    q_input.put((i, doc))

                with mp.Pool(thread_no) as pool:
                    res = pool.map_async(_asynch_download, args)

                    res_size = 0

                    for i in range(len(qs_output)):
                        doc = qs_output[i].get(timeout=60)
                        res_size += len(doc.page_content.encode("utf-8"))

                        if res_size > size_limit:
                            return

                        yield doc

                    # should return nearly immediately, but in case something would block, the timeout is set
                    res.wait(60)

            case _:  # "sequential" - simple sequential downloading
                res_size = 0
                for doc in sampled_docs:
                    doc.download()

                    loaded_doc = TextLoader(doc).load()[0]
                    res_size += len(loaded_doc.page_content.encode("utf-8"))

                    if res_size > size_limit:
                        return

                    yield loaded_doc
